import numpy as np
import joblib
import os
import gc

# ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from xgboost import XGBRegressor
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score


# -------------------------------------------------------
# [ë‚´ë¶€ í•¨ìˆ˜] íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ ë¹Œë”
# -------------------------------------------------------
def build_transformer_model(input_shape, head_size=64, num_heads=4, ff_dim=64, num_transformer_blocks=2, mlp_units=[64],
                            dropout=0.1, mlp_dropout=0.1):
    inputs = layers.Input(shape=input_shape)
    x = inputs

    # íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë¸”ë¡ ë°˜ë³µ
    for _ in range(num_transformer_blocks):
        # 1. Multi-Head Attention
        x_att = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
        x = layers.Add()([x, x_att])  # Residual Connection (ì…ë ¥ + ì–´í…ì…˜ ê²°ê³¼)
        x = layers.LayerNormalization(epsilon=1e-6)(x)

        # 2. Feed Forward Network (Conv1D ì‚¬ìš©)
        x_ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation="relu")(x)
        x_ff = layers.Dropout(dropout)(x_ff)
        x_ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x_ff)

        x = layers.Add()([x, x_ff])  # Residual Connection
        x = layers.LayerNormalization(epsilon=1e-6)(x)

    # ì¶œë ¥ í—¤ë“œ (Regression)
    x = layers.GlobalAveragePooling1D(data_format="channels_last")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation="relu")(x)
        x = layers.Dropout(mlp_dropout)(x)

    outputs = layers.Dense(1)(x)  # RTT ì˜ˆì¸¡ (1ê°œì˜ ê°’)

    return Model(inputs, outputs)

def train_data(model_name):
    model_name = model_name.lower()
    print(f"\n{'=' * 40}")
    print(f"ğŸš€ [{model_name.upper()}] í•™ìŠµ í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    print(f"{'=' * 40}")

    # -----------------------------------------
    # 1. XGBoost í•™ìŠµ ë¡œì§
    # -----------------------------------------

    if model_name == "xgboost":
        print(f"\n{'=' * 40}")
        print(f"ğŸ” [XGBoost] GridSearchCV íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
        print(f"{'=' * 40}")

        # 1. ë°ì´í„° ë¡œë“œ (ë©”ëª¨ë¦¬ ì•ˆì „ ëª¨ë“œ)
        try:
            print(" - [1/4] ë°ì´í„° ë¡œë“œ ë° ê²½ëŸ‰í™”(float32) ì¤‘...")
            X_train = np.load('X_train_xgb.npy').astype(np.float32)
            y_train = np.load('y_train.npy').astype(np.float32)
            X_test = np.load('X_test_xgb.npy').astype(np.float32)
            y_test = np.load('y_test.npy').astype(np.float32)
            print(f"   -> ë¡œë“œ ì™„ë£Œ: {X_train.shape}")
        except FileNotFoundError:
            print(f"[ì˜¤ë¥˜] ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # 2. íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ ì„¤ì •
        # (ë²”ìœ„ë¥¼ ë„ˆë¬´ ë„“ê²Œ ì¡ìœ¼ë©´ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ í•µì‹¬ êµ¬ê°„ ìœ„ì£¼ë¡œ ì„¤ì •)
        param_grid = {
            'n_estimators': [100, 300, 500],  # íŠ¸ë¦¬ ê°œìˆ˜
            'max_depth': [4, 6, 8],  # íŠ¸ë¦¬ ê¹Šì´
            'learning_rate': [0.05, 0.1],  # í•™ìŠµë¥ 
            'subsample': [0.8, 1.0]  # ë°ì´í„° ìƒ˜í”Œë§ ë¹„ìœ¨
        }

        print(f" - [2/4] íƒìƒ‰í•  íŒŒë¼ë¯¸í„° ì¡°í•© ì„¤ì • ì™„ë£Œ")
        print(f"   -> {param_grid}")

        # 3. GridSearchCV ì„¤ì • ë° í•™ìŠµ
        # ì£¼ì˜: n_jobs=-1ì„ GridCVì— ì£¼ë©´ ë°ì´í„° ë³µì‚¬ë³¸ì´ ìƒê²¨ ë©”ëª¨ë¦¬ í„°ì§ˆ ìˆ˜ ìˆìŒ.
        # ë”°ë¼ì„œ ëª¨ë¸(xgb)ì—ë§Œ n_jobs=-1ì„ ì£¼ê³ , GridCVëŠ” ìˆœì°¨ì (n_jobs=1)ìœ¼ë¡œ ëŒë¦¬ëŠ” ê²Œ ì•ˆì „í•¨.
        xgb = XGBRegressor(random_state=42, n_jobs=-1)

        grid_search = GridSearchCV(
            estimator=xgb,
            param_grid=param_grid,
            scoring='neg_mean_squared_error',  # íšŒê·€ì´ë¯€ë¡œ MSE ê¸°ì¤€ (ìŒìˆ˜ê°’ ì‚¬ìš©)
            cv=3,  # 3-Fold êµì°¨ ê²€ì¦
            verbose=2,  # ì§„í–‰ìƒí™© ì¶œë ¥
            n_jobs=1  # ë©”ëª¨ë¦¬ ì•ˆì „ì„ ìœ„í•´ 1ë¡œ ì„¤ì • (ë³‘ë ¬ì²˜ë¦¬ëŠ” xgbê°€ ìˆ˜í–‰)
        )

        print(" - [3/4] Grid Search í•™ìŠµ ì‹œì‘ (ì‹œê°„ì´ ë‹¤ì†Œ ì†Œìš”ë©ë‹ˆë‹¤)...")
        grid_search.fit(X_train, y_train)

        # 4. ìµœì  ê²°ê³¼ ë„ì¶œ ë° ë©”ëª¨ë¦¬ ì •ë¦¬
        best_xgb = grid_search.best_estimator_
        print(f"\nâœ¨ ìµœì  íŒŒë¼ë¯¸í„° ë°œê²¬: {grid_search.best_params_}")

        # í•™ìŠµ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ
        del X_train, y_train
        gc.collect()
        print("   -> í•™ìŠµ ë°ì´í„° ë©”ëª¨ë¦¬ í•´ì œ ì™„ë£Œ")

        # 5. ìµœì¢… í‰ê°€ ë° ì €ì¥
        print(" - [4/4] ìµœì¢… ëª¨ë¸ í‰ê°€ ë° ì €ì¥ ì¤‘...")
        y_pred = best_xgb.predict(X_test)

        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"ğŸ“Š [GridCV ìµœì¢… ê²°ê³¼] MSE: {mse:.4f}, R2: {r2:.4f}")

        # ëª¨ë¸ ì €ì¥
        joblib.dump(best_xgb, 'model_xgboost_best.joblib')
        print("ğŸ’¾ ìµœì  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model_xgboost_best.joblib")

    # -----------------------------------------
    # 2. LSTM í•™ìŠµ ë¡œì§
    # -----------------------------------------
    elif model_name == "lstm":
        # ë°ì´í„° ë¡œë“œ
        try:
            X_train = np.load('X_train_lstm.npy')
            y_train = np.load('y_train.npy')
            X_test = np.load('X_test_lstm.npy')
            y_test = np.load('y_test.npy')
        except FileNotFoundError:
            print(f"[ì˜¤ë¥˜] ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. process_data.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            return

        print(f" - ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {X_train.shape}")

        # ëª¨ë¸ êµ¬ì„±
        model = Sequential()
        # ì…ë ¥ í˜•íƒœ ìë™ ì¸ì‹ (TimeSteps, Features)
        model.add(LSTM(64, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))
        model.add(Dropout(0.2))
        model.add(Dense(32, activation='relu'))
        model.add(Dense(1))  # íšŒê·€ ì¶œë ¥ì„ ìœ„í•œ ë…¸ë“œ 1ê°œ

        model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))

        # ì½œë°± ì„¤ì • (ê³¼ì í•© ë°©ì§€ ë° ëª¨ë¸ ì €ì¥)
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),
            ModelCheckpoint('model_lstm_best.keras', monitor='val_loss', save_best_only=True)
        ]

        print(" - í•™ìŠµ ì‹œì‘ (Early Stopping ì ìš©)...")
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=50,  # ìµœëŒ€ 50ë²ˆ ëŒë˜
            batch_size=64,
            callbacks=callbacks,  # ì„±ëŠ¥ ì•ˆ ì˜¤ë¥´ë©´ ì¡°ê¸° ì¢…ë£Œ
            verbose=1
        )

        # í‰ê°€
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"ğŸ“Š [LSTM ê²°ê³¼] MSE: {mse:.4f}, R2: {r2:.4f}")
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model_lstm_best.keras")

        # ==========================================
        # 3. Transformer í•™ìŠµ (ì‹ ê·œ ì¶”ê°€)
        # ==========================================
    elif model_name == "transformer":
        # ë°ì´í„° ë¡œë“œ (LSTMê³¼ ë™ì¼í•œ 3D ë°ì´í„° ì‚¬ìš©)
        try:
            print(" - ë°ì´í„° ë¡œë“œ ì¤‘...")
            X_train = np.load('X_train_lstm.npy')
            y_train = np.load('y_train.npy')
            X_test = np.load('X_test_lstm.npy')
            y_test = np.load('y_test.npy')
        except FileNotFoundError:
            print(f"[ì˜¤ë¥˜] ë°ì´í„° íŒŒì¼(.npy)ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f" - ë°ì´í„° í˜•íƒœ: {X_train.shape}")  # (Samples, 60, 20)

        # ëª¨ë¸ ìƒì„±
        input_shape = (X_train.shape[1], X_train.shape[2])  # (60, 20)

        model = build_transformer_model(
            input_shape,
            head_size=64,  # ì–´í…ì…˜ í—¤ë“œ í¬ê¸°
            num_heads=4,  # ì–´í…ì…˜ í—¤ë“œ ê°œìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬)
            ff_dim=64,  # ë‚´ë¶€ í”¼ë“œí¬ì›Œë“œ ë§ í¬ê¸°
            num_transformer_blocks=2,  # ì¸ì½”ë” ë¸”ë¡ ì¸µ ìˆ˜ (ë„ˆë¬´ ê¹Šìœ¼ë©´ í•™ìŠµ ì–´ë ¤ì›€)
            mlp_units=[128],  # ìµœì¢… ì¶œë ¥ì¸µ ì „ì˜ Dense Layer
            dropout=0.1,
            mlp_dropout=0.1
        )

        model.compile(loss="mse", optimizer=Adam(learning_rate=0.001))

        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
            ModelCheckpoint('model_transformer_best.keras', monitor='val_loss', save_best_only=True)
        ]

        print(" - íŠ¸ëœìŠ¤í¬ë¨¸ í•™ìŠµ ì‹œì‘...")
        history = model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=50,  # í•„ìš”ì‹œ ëŠ˜ë¦¬ì„¸ìš”
            batch_size=64,
            callbacks=callbacks,
            verbose=1
        )

        # í‰ê°€
        y_pred = model.predict(X_test)
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)

        print(f"ğŸ“Š [Transformer ê²°ê³¼] MSE: {mse:.4f}, R2: {r2:.4f}")
        print("ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: model_transformer_best.keras")

    else:
        print(f"[ì˜¤ë¥˜] ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤: {model_name}.")